{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupational-garden",
   "metadata": {},
   "source": [
    "# Spark RDD\n",
    "-  스파크는 주 언어가 Scala임.\n",
    "- 기본적으로 클러스터 환경에서 동작하는 프로그램이기 때문에 AWS, Microsoft Azure 혹은 GCP(Google Cloud Platform) 등의 클라우드 환경에서 주로 사용.\n",
    "- 스파크의 경우 파이썬을 지원하기 때문에 몇 가지 기본 개념은 실습이 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-roberts",
   "metadata": {},
   "source": [
    "##  탄력적 분산 데이터셋 (RDD: Resilient Distributed Dataset)\n",
    "- 의미 :  스파크에서 사용하는 기본 추상개념으로 클러스터의 머신(노드)의 여러 메모리에 분산하여 저장할 수 있는 데이터의 집합.\n",
    "- 스파크는 RDD(Resilient Distributed Dataset)를 구현하기 위한 프로그램.\n",
    "- RDD를 스파크라는 프로그램을 통해 실행 시킴으로써 메모리 기반의 대량의 데이터 연산이 가능하게 되었고 이는 하둡보다 100배는 빠른 연산을 가능하게 해 주었음.\n",
    "- 스파크는 하드디스크에서 파일을 읽어온 뒤 연산 단계에는 데이터를 메모리에 저장하자는 아이디어를 생각함.\n",
    "- 메모리에 적재하기 좋은 새로운 형태의 추상화 작업(abstraction)이 필요하기 때문에 고안된 것이 바로 RDD(Resilient Distributed Dataset), \"탄력적 분산 데이터 셋\"임.\n",
    "- 데이터의 유실을 막기 위해 메모리의 데이터를 읽기 전용(Read-Only, 변경 불가)로 만들고 데이터를 만드는 방법(계보, Lineage)을 기록하고 있다가 데이터가 유실되면 다시 만드는 방법을 사용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "junior-headline",
   "metadata": {},
   "source": [
    "### RDD 특징\n",
    "- In-Memory\n",
    "- Fault Tolerance\n",
    "- Immutable(Read-Only)\n",
    "- Partition\n",
    "\n",
    "- 각 파티션은 RDD의 전체 데이터 중 일부를 나타냄.\n",
    "- 스파크는 데이터를 여러 대의 머신에 분할해서 저장하며, Chunk, 혹은 파티션으로 분할되어 저장함.\n",
    "- 파티션을 RDD데이터의 부분을 표현하는 단위 정도로 이해하면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-climate",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "- 내부에서 만들어진 데이터 집합을 병렬화하는 방법: parallelize()함수 사용\n",
    "- 외부의 파일을 로드하는 방법: .textFile() 함수 사용\n",
    "- 이 작업은 실제로는 RDD의 lineage(계보)를 만드는데 지나지 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-senior",
   "metadata": {},
   "source": [
    "### - RDD 동작\n",
    "- Transformations\n",
    "- Actions\n",
    "- RDD는 immutable(불변)하다고 하기 때문에 연산 수행에 있어 기존의 방식과는 다르게 수행됨. \n",
    "- Transformations은 RDD에게 변형 방법(연산 로직, 계보, lineage)을 알려주고 새로운 RDD를 만들지만 실제 연산의 수행은 Actions을 통해 행해짐.\n",
    "- Transformations를 통해 새로운 RDD를 만듦.\n",
    "- actions은 결괏값을 보여주고 저장하는 역할을 하며 실제 Transformations 연산을 지시하기도 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-palestine",
   "metadata": {},
   "source": [
    "## PySpark 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "coordinate-japan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-campbell",
   "metadata": {},
   "source": [
    "## SparkContext를 통한 스파크 초기화\n",
    "- 분산 환경에서 운영되는 스파크는 driver 프로그램을 구동시킬 때 SparkContext라는 특수 객체를 만들게 됨.\n",
    "- 스파크는 이 SparkContext 객체를 통해 스파크의 모든 기능에 접근함.\n",
    "- 이 객체는 스파크 프로그램당 한 번만 실행할 수 있고 사용 후에는 종료해야 함.\n",
    "- SparkContext를 다른 말로 스파크의 \"엔트리 포인트(entry point)\"라고도 하고, SparkContext를 생성하는 것을 \"스파크를 초기화한다(Initializing Spark)\"라고 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-slope",
   "metadata": {},
   "source": [
    "### - PySpark에서\n",
    "- 문법: pyspark.SparkContext()\n",
    "- 스파크 기능의 기본 엔트리 포인트입니다.\n",
    "- 스파크 클러스터와 연결을 나타내며 RDD를 만들고 브로드캐스트 하는데 사용될 수 있습니다.\n",
    "- JVM 당 하나만 활성화해야 하며, 새로운 것을 만들기 전에는 활성을 중지해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decimal-gross",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w0ul9i4kxtvxj63dy3wsu0dmb-7c7b5bbc6c-57m57:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "#보통 변수명은  sc로 선언함.\n",
    "sc = SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "moving-guatemala",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkContext 종료\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cubic-identity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w0ul9i4kxtvxj63dy3wsu0dmb-7c7b5bbc6c-57m57:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = SparkContext(master='local', appName='PySpark Basic')\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "worldwide-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.name', 'PySpark Basic'),\n",
       " ('spark.app.id', 'local-1630645115691'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.port', '44001'),\n",
       " ('spark.driver.host', 'w0ul9i4kxtvxj63dy3wsu0dmb-7c7b5bbc6c-57m57'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#생성한 SparkContext의 Configuration을 확인\n",
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "animated-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "compact-banana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PySpark Basic'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.appName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "appropriate-tampa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#종료\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "romance-requirement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://w0ul9i4kxtvxj63dy3wsu0dmb-7c7b5bbc6c-57m57:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Basic</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=PySpark Basic>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.setMaster(), .setAppName()을 이용해 어플리케이션의 이름과 Master의 URL을 설정\n",
    "conf = SparkConf().setAppName('PySpark Basic').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-prototype",
   "metadata": {},
   "source": [
    "## RDD Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "refined-permit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#방금 전 만든 SparkContext()의 parallelize()함수를 이용해서 내부의 데이터 집합을 RDD로 만듦\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "controlling-collective",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-question",
   "metadata": {},
   "source": [
    "- 지금은 RDD를 생성했지만 RDD가 만들어지지는 않은 것."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dominant-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RDD 원소 반환\n",
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "technical-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "#외부 파일 로드하여 RDD 만들기\n",
    "import os\n",
    "\n",
    "file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/test.txt'\n",
    "with open(file_path, 'w') as f:\n",
    "    for i in range(10):\n",
    "        f.write(str(i)+'\\n')\n",
    "        \n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chicken-classroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/bigdata_ecosystem/test.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "rdd2 = sc.textFile(file_path)\n",
    "print(rdd2)\n",
    "print(type(rdd2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "preliminary-motel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.take(3)\n",
    "# spark가 .textFile()을 통해 얻어온 데이터 타입을 무조건 string으로 처리함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-prefix",
   "metadata": {},
   "source": [
    "## RDD Operation - Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecological-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'a', 'c', 'd']\n",
      "[('b', 1), ('a', 1), ('c', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "#map\n",
    "x = sc.parallelize([\"b\", \"a\", \"c\", \"d\"])\n",
    "y = x.map(lambda z: (z, 1))\n",
    "print(x.collect()) #collect()는 actions \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "detected-intranet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3])\n",
    "squares = nums.map(lambda x: x*x)\n",
    "print(squares.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "intensive-switch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "#filter\n",
    "x = sc.parallelize([1,2,3,4,5])\n",
    "y = x.filter(lambda x: x%2 == 0) \n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "artistic-price",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd']\n",
      "['A']\n"
     ]
    }
   ],
   "source": [
    "text = sc.parallelize(['a', 'b', 'c', 'd'])\n",
    "capital = text.map(lambda x: x.upper())\n",
    "A = capital.filter(lambda x: 'A' in x)\n",
    "print(text.collect())\n",
    "print(A.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "exciting-consortium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 10, 30, 2, 20, 30, 3, 30, 30]\n"
     ]
    }
   ],
   "source": [
    "#flatmap\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, x*10, 30))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "impossible-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark',\n",
       " 'is',\n",
       " 'funny',\n",
       " 'it',\n",
       " 'is',\n",
       " 'beautiful',\n",
       " 'and',\n",
       " 'also',\n",
       " 'it',\n",
       " 'is',\n",
       " 'implemented',\n",
       " 'by',\n",
       " 'python']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDataset = sc.parallelize([\"Spark is funny\", \"It is beautiful\", \"And also It is implemented by python\"])\n",
    "result = wordsDataset.flatMap(lambda x: x.split()).filter(lambda x: x != \" \").map(lambda x: x.lower())\n",
    "#공백은 제거\n",
    "#단어를 공백기준으로 split.\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "noted-history",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survived,sex,age,n_siblings_spouses,parch,fare,class,deck,embark_town,alone',\n",
       " '0,male,22.0,1,0,7.25,Third,unknown,Southampton,n',\n",
       " '1,female,38.0,1,0,71.2833,First,C,Cherbourg,n',\n",
       " '1,female,26.0,0,0,7.925,Third,unknown,Southampton,y',\n",
       " '1,female,35.0,1,0,53.1,First,C,Southampton,n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#응용 - csv 파일 읽기\n",
    "import os\n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_0.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "executive-chile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#비어있는 라인은 제외하고, delimeter인 ,로 line을 분리\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "csv_data_1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sunrise-coating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['survived',\n",
       "  'sex',\n",
       "  'age',\n",
       "  'n_siblings_spouses',\n",
       "  'parch',\n",
       "  'fare',\n",
       "  'class',\n",
       "  'deck',\n",
       "  'embark_town',\n",
       "  'alone']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#칼럼 부분만 분리\n",
    "columns = csv_data_1.take(1)\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "heated-rating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  'male',\n",
       "  '22.0',\n",
       "  '1',\n",
       "  '0',\n",
       "  '7.25',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'n'],\n",
       " ['1', 'female', '38.0', '1', '0', '71.2833', 'First', 'C', 'Cherbourg', 'n'],\n",
       " ['1',\n",
       "  'female',\n",
       "  '26.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '7.925',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Southampton',\n",
       "  'y'],\n",
       " ['1', 'female', '35.0', '1', '0', '53.1', 'First', 'C', 'Southampton', 'n'],\n",
       " ['0',\n",
       "  'male',\n",
       "  '28.0',\n",
       "  '0',\n",
       "  '0',\n",
       "  '8.4583',\n",
       "  'Third',\n",
       "  'unknown',\n",
       "  'Queenstown',\n",
       "  'y']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#칼럼 제외한 나머지 데이터만 분리\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())  # 첫 번째 컬럼이 숫자인 것만 필터링\n",
    "csv_data_2.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coated-warrior",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '35.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '53.1'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '28.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '8.4583'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Queenstown'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#칼럼 기준으로 정리\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "csv_data_3.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "jewish-solid",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class|deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "|0       |male  |22.0|1                 |0    |7.25   |Third|unknown|Southampton|n    |\n",
      "|1       |female|38.0|1                 |0    |71.2833|First|C      |Cherbourg  |n    |\n",
      "|1       |female|26.0|0                 |0    |7.925  |Third|unknown|Southampton|y    |\n",
      "|1       |female|35.0|1                 |0    |53.1   |First|C      |Southampton|n    |\n",
      "|0       |male  |28.0|0                 |0    |8.4583 |Third|unknown|Queenstown |y    |\n",
      "+--------+------+----+------------------+-----+-------+-----+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#csv를 DataFrame으로 읽어 들이는 방법\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark import SparkFiles\n",
    "\n",
    "url = 'https://storage.googleapis.com/tf-datasets/titanic/train.csv'\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sqlContext.read.csv(SparkFiles.get(\"train.csv\"), header=True, inferSchema= True)\n",
    "df.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "elementary-local",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|survived|sex   |age |n_siblings_spouses|parch|fare   |class |deck   |embark_town|alone|\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "|0       |male  |66.0|0                 |0    |10.5   |Second|unknown|Southampton|y    |\n",
      "|0       |male  |42.0|1                 |0    |52.0   |First |unknown|Southampton|n    |\n",
      "|1       |female|49.0|1                 |0    |76.7292|First |D      |Cherbourg  |n    |\n",
      "|0       |male  |65.0|0                 |1    |61.9792|First |B      |Cherbourg  |n    |\n",
      "|0       |male  |45.0|1                 |0    |83.475 |First |C      |Southampton|n    |\n",
      "+--------+------+----+------------------+-----+-------+------+-------+-----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#위에서 얻은 데이터에서 40세 이상인 사람들의 데이터만 필터링. \n",
    "df2 = df[df['age']>40]\n",
    "df2.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-style",
   "metadata": {},
   "source": [
    "## RDD Operation - Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "imperial-location",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect\n",
    "nums = sc.parallelize(list(range(10)))\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adopted-teacher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take\n",
    "nums.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "stable-architecture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count\n",
    "nums.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deadly-heritage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce\n",
    "nums.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "governing-throw",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#saveAsTextFile\n",
    "#file_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/file.txt'\n",
    "#nums.saveAsTextFile(file_path)\n",
    "\n",
    "#!ls -l ~/aiffel/bigdata_ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-composition",
   "metadata": {},
   "source": [
    "- file.txt 내용을 잘 보시면, 놀랍게도 디렉토리 타입으로 생성되어 있음.\n",
    "- 이 디렉토리 안에 들어가 보면 part-00000 라는 이름의 텍스트 파일이 생성되어 있어서, 실제 우리가 기록하고 싶었던 내용은 이 파일 안에 있음.\n",
    "- 스파크가 다룰 파일 사이즈는 하드디스크 하나에 다 담지 못할 만큼 큰 경우일 수도 있지만, 비록 작은 RDD지만 우리는 이미 sc.parallelize()를 통해 분산형 데이터로 생성했음을 잊으면 안됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "greenhouse-command",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1580.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 생성\n",
    "rdd = sc.parallelize(range(1,100))\n",
    "\n",
    "# RDD Transformation \n",
    "rdd2 = rdd.map(lambda x: 0.5*x - 10).filter(lambda x: x > 0)\n",
    "\n",
    "# RDD Action \n",
    "rdd2.reduce(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-theta",
   "metadata": {},
   "source": [
    "## RDD Operation - MapReduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-authorization",
   "metadata": {},
   "source": [
    "### Word Counter 구현하기\n",
    "- Map 함수를 구현할 때, 입력 스트링의 각 문자 x에 대해 x -> (x, 1) 형태의 tuple로 매핑.\n",
    "- 반적인 reduce 함수보다는 reduceByKey 함수를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "portuguese-gothic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 2),\n",
       " ('e', 1),\n",
       " ('l', 2),\n",
       " ('o', 2),\n",
       " ('p', 1),\n",
       " ('y', 1),\n",
       " ('t', 1),\n",
       " ('n', 1)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = sc.parallelize('hello python')\n",
    "\n",
    "# map 함수를 적용한 RDD 구하기\n",
    "text_1 = text.filter(lambda x: x != \" \")\n",
    "text_2 = text_1.map(lambda x:(x, 1))\n",
    "\n",
    "#reduceByKey 함수를 적용한 Word Counter 출력\n",
    "word_count = text_2.reduceByKey(lambda accum, n: accum + n)  \n",
    "word_count.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-postcard",
   "metadata": {},
   "source": [
    "### Titanic 데이터 분석하기\n",
    "- map 함수를 이용해 모든 데이터를 (생존 여부, 연령)의 형태로 바꾸면 생존자, 사망자 각각의 연령의 총합을 쉽게 구할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "indie-chest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('survived', '0'),\n",
       "  ('sex', 'male'),\n",
       "  ('age', '22.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.25'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '38.0'),\n",
       "  ('n_siblings_spouses', '1'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '71.2833'),\n",
       "  ('class', 'First'),\n",
       "  ('deck', 'C'),\n",
       "  ('embark_town', 'Cherbourg'),\n",
       "  ('alone', 'n')],\n",
       " [('survived', '1'),\n",
       "  ('sex', 'female'),\n",
       "  ('age', '26.0'),\n",
       "  ('n_siblings_spouses', '0'),\n",
       "  ('parch', '0'),\n",
       "  ('fare', '7.925'),\n",
       "  ('class', 'Third'),\n",
       "  ('deck', 'unknown'),\n",
       "  ('embark_town', 'Southampton'),\n",
       "  ('alone', 'y')]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이전 스텝에서 CSV 파일을 로딩했던 내역입니다. \n",
    "csv_path = os.getenv('HOME')+'/aiffel/bigdata_ecosystem/train.csv'\n",
    "csv_data_0 = sc.textFile(csv_path)\n",
    "csv_data_1 = csv_data_0.filter(lambda line: len(line)>1).map(lambda line: line.split(\",\"))   \n",
    "columns = csv_data_1.take(1)\n",
    "csv_data_2 = csv_data_1.filter(lambda line: line[0].isdecimal())\n",
    "csv_data_3 = csv_data_2.map(lambda line: [(columns[0][i], linedata) for i, linedata in enumerate(line)])\n",
    "\n",
    "csv_data_3.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "smooth-washington",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생존자 평균 연령: 29.110411522633743\n",
      "사망자 평균 연령: 29.9609375\n"
     ]
    }
   ],
   "source": [
    "#생존자와 사망자의 연령 총합 구하기\n",
    "csv_data_4 = csv_data_3.map(lambda line:(line[0][1], line[2][1]))   # (생존여부, 연령)\n",
    "age_sum_data = csv_data_4.reduceByKey(lambda accum, age: float(accum) + float(age))  \n",
    "age_sum = age_sum_data.collect()\n",
    "\n",
    "#생존자와 사망자의 사람 수 구하기\n",
    "csv_data_5 = csv_data_3.map(lambda line:(line[0][1], 1))\n",
    "survived_data = csv_data_5.reduceByKey(lambda accum, count: int(accum) + int(count)) \n",
    "survived_count = survived_data.collect()\n",
    "\n",
    "age_sum_dict = dict(age_sum)\n",
    "survived_dict = dict(survived_count)\n",
    "avg_age_survived = age_sum_dict['1']/survived_dict['1']\n",
    "print('생존자 평균 연령:' ,avg_age_survived)\n",
    "avg_age_died = age_sum_dict['0']/survived_dict['0']\n",
    "print('사망자 평균 연령:' ,avg_age_died)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
